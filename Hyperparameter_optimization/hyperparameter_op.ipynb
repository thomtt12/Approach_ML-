{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Optimizing hyper-paramters to get the best scoring model. \r\n",
    "The parameters that control the training/fitting process of the model\r\n",
    "---\r\n",
    "Example: Train a linear regression with SGD, **parameters** of a model are the slope and the bias; **hyperparameter** is learning rate. There are 3 parameters a, b,c in the model; these parameters can be integers between 1 and 10.\r\n",
    "A combination of these parameters will provide best result. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# define the best accuracy to be 0\r\n",
    "# if you choose loss as a metric,\r\n",
    "# you can make best loss to be inf (np.inf)\r\n",
    "best_accuracy = 0\r\n",
    "best_parameters = {\"a\": 0, \"b\": 0, \"c\": 0}\r\n",
    "# loop over all values for a, b & c\r\n",
    "for a in range(1, 11):\r\n",
    "    for b in range(1, 11):\r\n",
    "        for c in range(1, 11):\r\n",
    "            # inititalize model with current parameters\r\n",
    "            model = MODEL(a, b, c)\r\n",
    "            # fit the model\r\n",
    "            model.fit(training_data)\r\n",
    "            # make predictions\r\n",
    "            preds = model.predict(validation_data)\r\n",
    "            # calculate accuracy\r\n",
    "            accuracy = metrics.accuracy_score(targets, preds)\r\n",
    "            # save params if current accuracy\r\n",
    "            # is greater than best accuracy\r\n",
    "            if accuracy > best_accuracy:\r\n",
    "                best_accuracy = accuracy\r\n",
    "                best_parameters[\"a\"] = a\r\n",
    "                best_parameters[\"b\"] = b\r\n",
    "                best_parameters[\"c\"] = c\r\n",
    "\r\n",
    "\"\"\"\r\n",
    "Cons: go through all the para from 1 to 10 >> have 1000 (10x10x10) fits for the model >> take a long time to train \r\n",
    "Real world, parameters are real-valued(not only ten values for each para) and the combinations of different para can be infinite\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# rf_grid_search.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# rf_random_search.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random search is faster than grid search if the number of iterations is less.\r\n",
    "Example: multiclass classification problem. Training data consists of two text columns. >> require to build a model to predict the class. \r\n",
    "Assume: the pipeline is first apply tf-idf in a semisupervised manner and then use SVD with SVM classifier. \r\n",
    ">> ?? how to select the components of SVD, tune the parameters of SVM...\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#pipeline_search.py\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#rf_gp_minimize.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#rf_hyperopt.py"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " The ways of tuning hyperparameters described above will work with almost all models: linear regression,\r\n",
    "logistic regression, tree-based methods, gradient boosting models such as xgboost,\r\n",
    "lightgbm, and even neural networks...\r\n",
    "\r\n",
    "To learn, must start with tuning the hyperpara manually (ex: by hand). Hand tuning will help u learn basics\r\n",
    "examples: gradient boosting, when increase the depth, should reduce the learning rate. \r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When you create large models or introduce a lot of features, you also make it susceptible to overfitting the training data\r\n",
    "To avoid overfitting,  need to introduce noise in training data features or penalize the cost function. \r\n",
    "This penalization is called **regularization** and helps with generalizing the model\r\n",
    "\r\n",
    "Example: \r\n",
    "In linear models, the most common types of regularizations are L1 and L2. L1 is also known as Lasso regression and L2 as Ridge regression\r\n",
    "\r\n",
    "In neural networks, use dropouts, the addition of augmentations, noise, etc. to regularize our models. Using hyper-parameter optimization, can find the correct penalty to use"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "9f54772c6dbcef3e2bbb3b58a12d0ea93cc39e8d73ca8d1792b5e194a83990cc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}