{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dua/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"fishing\", \"fishes\", \"fished\"]\n",
    "for word in words:\n",
    "    print(f\"word={word}\")\n",
    "    print(f\"stemmed_word={stemmer.stem(word)}\")\n",
    "    print(f\"lemma={lemmatizer.lemmatize(word)}\")\n",
    "    print(\"\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word=fishing\n",
      "stemmed_word=fish\n",
      "lemma=fishing\n",
      "\n",
      "word=fishes\n",
      "stemmed_word=fish\n",
      "lemma=fish\n",
      "\n",
      "word=fished\n",
      "stemmed_word=fish\n",
      "lemma=fished\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create a corpus of sentences\n",
    "# read only 10k samples from training data\n",
    "\n",
    "corpus = pd.read_csv(\"/home/dua/Documents/text_classify_regression/input /IMDB Dataset.csv\", nrows=10000)\n",
    "corpus = corpus.review.values\n",
    "# initialize TfidfVectorizer with word_tokenize from nltk as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "# transform the corpus using tfidf\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "# initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "# fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "# choose first sample and create a dictionary of feature names and their scores from svd\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "# once have the dictionary, can sort it in decreasing order and get the top N topics\n",
    "N = 5\n",
    "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', ',', '.', 'and', 'a']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\"\"\"\n",
    "Clean any text data, especially when it's in pandas dataframe. \n",
    "example: convert a string like \"hi, how are you???\" to \"hi how are you\"\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import string\n",
    "def clean_text(s):\n",
    "    \"\"\"\n",
    "    This function cleans the text a bit\n",
    "    s: string\n",
    "    return: cleaned string\n",
    "    \"\"\"\n",
    "    # split by all whitespaces\n",
    "    s = s.split()\n",
    "    # join tokens by single space >>  remove all kinds of weird space\n",
    "    # \"hi.   how are you\" becomes \"hi. how are you\"\n",
    "    s = \" \".join(s)\n",
    "    # remove all punctuations using regex and string module\n",
    "    s = re.sub(f'[{re.escape(string.punctuation)}]', '', s)\n",
    "    # you can add more cleaning here if you want\n",
    "    # and then return the cleaned string\n",
    "    return s"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "apply above function to the old SVD code \n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create a corpus of sentences\n",
    "# read only 10k samples from training data\n",
    "\n",
    "corpus = pd.read_csv(\"/home/dua/Documents/text_classify_regression/input /IMDB Dataset.csv\", nrows=10000)\n",
    "\n",
    "corpus.loc[:, \"review\"] = corpus.review.apply(clean_text)\n",
    "\n",
    "# initialize TfidfVectorizer with word_tokenize from nltk as the tokenizer\n",
    "tfv = TfidfVectorizer(tokenizer=word_tokenize, token_pattern=None)\n",
    "# fit the vectorizer on corpus\n",
    "tfv.fit(corpus)\n",
    "# transform the corpus using tfidf\n",
    "corpus_transformed = tfv.transform(corpus)\n",
    "# initialize SVD with 10 components\n",
    "svd = decomposition.TruncatedSVD(n_components=10)\n",
    "# fit SVD\n",
    "corpus_svd = svd.fit(corpus_transformed)\n",
    "# choose first sample and create a dictionary of feature names and their scores from svd\n",
    "sample_index = 0\n",
    "feature_scores = dict(\n",
    "    zip(\n",
    "        tfv.get_feature_names(),\n",
    "        corpus_svd.components_[sample_index]\n",
    "    )\n",
    ")\n",
    "# once have the dictionary, can sort it in decreasing order and get the top N topics\n",
    "N = 5\n",
    "print(sorted(feature_scores, key=feature_scores.get, reverse=True)[:N])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#sentence vector \n",
    "import numpy as np\n",
    "def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n",
    "    \"\"\"\n",
    "    Given a sentence and other information, this function returns embedding for the whole sentence\n",
    "    s: sentence, string\n",
    "    embedding_dict: dictionary word:vector\n",
    "    stop_words: list of stop words, if any\n",
    "    tokenizer: a tokenization function\n",
    "    \"\"\"\n",
    "    # convert sentence to string and lowercase it\n",
    "    words = str(s).lower()\n",
    "    # tokenize the sentence\n",
    "    words = tokenizer(words)\n",
    "    # remove stop word tokens\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # keep only alpha-numeric tokens\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    # initialize empty list to store embeddings\n",
    "    M = []\n",
    "    for w in words:\n",
    "        # for evert word, fetch the embedding from the dictionary and append to list of embeddings\n",
    "        if w in embedding_dict:\n",
    "            M.append(embedding_dict[w])\n",
    "    # if we dont have any vectors, return zeros\n",
    "    if len(M) == 0:\n",
    "        return np.zeros(300)\n",
    "    # convert list of embeddings to array\n",
    "    M = np.array(M)\n",
    "    # calculate sum over axis=0\n",
    "    v = M.sum(axis=0)\n",
    "    # return normalized vector\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "0e8ca4fea3fc44f084e5a88307ff027a4084c99ef5cf79f6c4c2eadb45e0ef77"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}